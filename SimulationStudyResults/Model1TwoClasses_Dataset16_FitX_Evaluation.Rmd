---
title: "Table of contents"
output:
  html_document:
    toc: true
    toc_depth: 2
    toc_float: TRUE
---


```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE)
```


```{r, include = FALSE}
# README
# required files:
# Dataset16_Yobslog.xlsx
# Dataset16_zsim.xlsx
# Model1TwoClasses_Dataset16_Fit1.rds
```


# Color coding
* <span style="color: blueviolet;">blueviolet</span> for estimated parameters, also <span style="color: deepskyblue;">deepskyblue</span>
* <span style="color: deeppink;">deeppink</span> for simulated parameters
* <span style="color: darkturquoise;">darkturquoise</span> for predicted dependent variable
* <span style="color: orange;">orange</span> for observed dependent variable


# Labeling restriction
$\beta_{0,c} < \beta_{0,c+1}$


# Hyperparameters
* $\mu_{\beta_{0,c}} = \kappa_c$, where $\boldsymbol{\kappa}$ is a row vector of size $C$ storing the estimates of K-means clustering for $\boldsymbol{y}^{obs}_{t=1}$, with $\kappa_c \leq \kappa_{c+1}$; K-means clustering is performed using the algorithm of Hartigan and Wong (1979) with maximum ten iterations and two random sets

* $\sigma_{\beta_{0,c}} = 1$

* $\mu_{\beta_{1,c}} = 0$

* $\sigma_{\beta_{1,c}} = 1$

* $\sigma_{\sigma_c} = 0.5$


# NUTS parameters
* Number of chains: $4$

* Number of iterations per chain: $2000$

* Number of warmup iterations per chain: $1000$

* Number of sampling iterations per chain: $1000$

* Initial values for parameters: random initial values


```{r, include = FALSE}
# preparation ####
# set working directory
setwd("C:/Users/Diiim/Documents/GitHub/BayesianGMMs")

# set decimals to digits instead of scientific
options(scipen = 999)

# load packages
library(readxl)
library(rstan)
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)


# number of latent classes ####
C <- 2


# simulated data ####
# load observed dependent variable
Y_obs <- data.frame(read_excel("SimulationStudyData/Dataset16_Yobs.xlsx",
                               sheet = "Sheet 1"))
Y_obs_log <- data.frame(read_excel("SimulationStudyData/Dataset16_Yobslog.xlsx",
                                   sheet = "Sheet 1"))

# number of individuals
N <- dim(Y_obs)[1]

# number of time periods
no_periods <- dim(Y_obs)[2]

# time periods
time_periods <- 0:(no_periods-1)
X <- matrix(data = time_periods, nrow = N, ncol = no_periods, byrow = TRUE)

# simulated mixture proportions
lambda_sim <- c(0.3,0.7)

# load simulated class memberships
z_sim <- data.frame(read_excel("SimulationStudyData/Dataset16_zsim.xlsx",
                               sheet = "Sheet 1"))

# simulated constants
beta_0_sim <- c(1,5)

# simulated linear trend components
beta_1_sim <- c(0.4,0.1)


# estimated data ####
# load model fit
m_fit <- readRDS("SimulationStudyResults/Model1TwoClasses_Dataset16_Fit1.rds")

# extract estimated data
m_fit_data <- rstan::extract(m_fit)

# posterior for class memberships
z_posterior_n <- matrix(data = 0, nrow = C, ncol = 1)
z_posterior_n_rownames <- c()
for (c in 1:C) {
  z_posterior_n_rownames[c] <- paste("c =", c)
}
rownames(z_posterior_n) <- z_posterior_n_rownames
z_posterior <- list()
for (n in 1:N) {
  prop_table <- prop.table(table(factor(m_fit_data$z[,n], levels = 1:C)))
  z_posterior_n[,1] <- prop_table
  z_posterior[[n]] <- z_posterior_n
}

# predicted dependent variable
Y_pred_log <- m_fit_data$Y_pred

# middle 90% credible intervals for Y_pred_log
Y_pred_log_CI_lower <- matrix(data = 0, nrow = N, ncol = no_periods)
Y_pred_log_CI_upper <- matrix(data = 0, nrow = N, ncol = no_periods)
for (t in 1:no_periods) {
  for (n in 1:N) {
    Y_pred_log_CI_lower[n,t] <- quantile(Y_pred_log[,n,t], prob = 0.05)
    Y_pred_log_CI_upper[n,t] <- quantile(Y_pred_log[,n,t], prob = 0.95)
  }
}

# exp transform Y_pred
Y_pred <- exp(Y_pred_log)

# middle 90% credible intervals for Y_pred
Y_pred_CI_lower <- matrix(data = 0, nrow = N, ncol = no_periods)
Y_pred_CI_upper <- matrix(data = 0, nrow = N, ncol = no_periods)
for (t in 1:no_periods) {
  for (n in 1:N) {
    Y_pred_CI_lower[n,t] <- quantile(Y_pred[,n,t], prob = 0.05)
    Y_pred_CI_upper[n,t] <- quantile(Y_pred[,n,t], prob = 0.95)
  }
}


# colors ####
darkturquoise_transp <- rgb(red = 0,
                            green = 206,
                            blue = 209,
                            max = 209,
                            alpha = 50,
                            names = "darkturquoise")
```


# Convergence
```{r, include = FALSE}
# diagnostics: Rhat, Bulk ESS, and Tail ESS for lambda, beta_0, beta_1, and
# sigma
params <- c("lambda[1]","lambda[2]",
            "beta_0[1]","beta_0[2]",
            "beta_1[1]","beta_1[2]",
            "sigma[1]","sigma[2]")
diagnostics <- c("Rhat","Bulk_ESS","Tail_ESS")
Rhat_ESS <-
  as.data.frame(monitor(extract(m_fit, permuted = FALSE)))[params,diagnostics]
```


```{r}
Rhat_ESS
```


```{r}
# diagnostics: trace plots for mixture proportions
for (c in 1:C) {
  plot(m_fit_data$lambda[,c],
       type = "l",
       col = "blueviolet",
       main = "",
       xlab = "",
       ylab = "")
  
  grid(nx = NA, ny = NULL)
  par(new = TRUE)
  
  plot(m_fit_data$lambda[,c],
       type = "l",
       col = "blueviolet",
       main = paste("Mixture proportion for c =", c),
       xlab = "Sampling iteration",
       ylab = "lambda")
  
  box()
}
```


```{r}
# diagnostics: trace plots for constants
for (c in 1:C) {
  plot(m_fit_data$beta_0[,c],
       type = "l",
       col = "blueviolet",
       main = "",
       xlab = "",
       ylab = "")
  
  grid(nx = NA, ny = NULL)
  par(new = TRUE)
  
  plot(m_fit_data$beta_0[,c],
       type = "l",
       col = "blueviolet",
       main = paste("Constant for c =", c),
       xlab = "Sampling iteration",
       ylab = "beta_0")
  
  box()
}
```


```{r}
# diagnostics: trace plots for linear trend components
for (c in 1:C) {
  plot(m_fit_data$beta_1[,c],
       type = "l",
       col = "blueviolet",
       main = "",
       xlab = "",
       ylab = "")
  
  grid(nx = NA, ny = NULL)
  par(new = TRUE)
  
  plot(m_fit_data$beta_1[,c],
       type = "l",
       col = "blueviolet",
       main = paste("Linear trend component for c =", c),
       xlab = "Sampling iteration",
       ylab = "beta_1")
  
  box()
}
```


```{r}
# diagnostics: trace plots for standard deviations for Y Normal distributions
for (c in 1:C) {
  plot(m_fit_data$sigma[,c],
       type = "l",
       col = "blueviolet",
       main = "",
       xlab = "",
       ylab = "")
  
  grid(nx = NA, ny = NULL)
  par(new = TRUE)
  
  plot(m_fit_data$sigma[,c],
       type = "l",
       col = "blueviolet",
       main = paste("Standard deviation of Y Normal distributions for c =", c),
       xlab = "Sampling iteration",
       ylab = "sigma")
  
  box()
}
```


# Posterior
```{r}
# mixture proportions
for (c in 1:C) {
  hist(m_fit_data$lambda[,c],
       xlim = c(min(m_fit_data$lambda[,c],lambda_sim[c]),
                max(m_fit_data$lambda[,c],lambda_sim[c])),
       col = "blueviolet",
       border = FALSE,
       main = "",
       xlab = "")
  
  grid()
  par(new = TRUE)

  hist(m_fit_data$lambda[,c],
       xlim = c(min(m_fit_data$lambda[,c],lambda_sim[c]),
                max(m_fit_data$lambda[,c],lambda_sim[c])),
       col = "blueviolet",
       border = FALSE,
       main = paste("Mixture proportion for c =", c),
       xlab = "lambda")
  
  
  abline(v = lambda_sim[c],
         lwd = 2,
         col = "deeppink")
  
  
  box()
}
```


```{r}
# constants
for (c in 1:C) {
  hist(m_fit_data$beta_0[,c],
       xlim = c(min(m_fit_data$beta_0[,c],beta_0_sim[c]),
                max(m_fit_data$beta_0[,c],beta_0_sim[c])),
       col = "blueviolet",
       border = FALSE,
       main = "",
       xlab = "")
  
  grid()
  par(new = TRUE)
  
  hist(m_fit_data$beta_0[,c],
       xlim = c(min(m_fit_data$beta_0[,c],beta_0_sim[c]),
                max(m_fit_data$beta_0[,c],beta_0_sim[c])),
       col = "blueviolet",
       border = FALSE,
       main = paste("Constant for c =", c),
       xlab = "beta_0")
  
  abline(v = beta_0_sim[c],
         lwd = 2,
         col = "deeppink")
  
  box()
}
```


```{r}
# linear trend components
for (c in 1:C) {
  hist(m_fit_data$beta_1[,c],
       xlim = c(min(m_fit_data$beta_1[,c],beta_1_sim[c]),
                max(m_fit_data$beta_1[,c],beta_1_sim[c])),
       col = "blueviolet",
       border = FALSE,
       main = "",
       xlab = "")
  
  grid()
  par(new = TRUE)
  
  hist(m_fit_data$beta_1[,c],
       xlim = c(min(m_fit_data$beta_1[,c],beta_1_sim[c]),
                max(m_fit_data$beta_1[,c],beta_1_sim[c])),
       col = "blueviolet",
       border = FALSE,
       main = paste("Linear trend component for c =", c),
       xlab = "beta_1")
  
  abline(v = beta_1_sim[c],
         lwd = 2,
         col = "deeppink")
  
  box()
}
```


```{r}
# standard deviations for Y Normal distributions
for (c in 1:C) {
  hist(m_fit_data$sigma[,c],
       col = "blueviolet",
       border = FALSE,
       main = "",
       xlab = "")
  
  grid()
  par(new = TRUE)
  
  hist(m_fit_data$sigma[,c],
       col = "blueviolet",
       border = FALSE,
       main = paste("Standard deviation of Y Normal distributions for c =", c),
       xlab = "sigma")
  
  box()
}
```


```{r}
# class memberships
for (n in 1:N) {
  barplot(z_posterior[[n]],
          beside = TRUE,
          col = c("deepskyblue","blueviolet"),
          border = FALSE,
          main = "",
          names.arg = "",
          ylab = "")
  
  grid()
  par(new = TRUE)
  
  barplot(z_posterior[[n]],
          beside = TRUE,
          col = c("deepskyblue","blueviolet"),
          border = FALSE,
          main = paste("Latent class membership for n =", n),
          names.arg = "",
          ylab = "Pr( z = c )",
          legend.text = rownames(z_posterior[[n]]))
  
  legend("top",
         legend = paste("z_sim =", z_sim[n,1]),
         text.col = "deeppink")
  
  box()
}
```


# Posterior predictive check
```{r}
# middle 90% credible intervals
for (n in 1:N) {
  x <- X[n,]
  
  # for Y_pred_log
  y_lower <- Y_pred_log_CI_lower[n,]
  y_upper <- Y_pred_log_CI_upper[n,]
  
  plot(x = x,
       y = c(min(y_lower),
             max(y_upper),
             min(Y_obs_log[n,]),
             max(Y_obs_log[n,]),
             rep(min(y_lower), times = no_periods-4)),
       type="l",
       col = "white",
       main = paste("Predicted dependent variable on log scale for n =", n),
       xlab = "x",
       ylab = "y_log")
  
  grid(nx = NA, ny = NULL)
  par(new = TRUE)
  
  polygon(x = c(x, rev(x)),
          y = c(y_lower, rev(y_upper)),
          col = darkturquoise_transp,
          lty = 0)
  
  lines(x = x,
        y = Y_obs_log[n,],
        lwd = 2,
        col = "orange")
  
  legend("top",
         legend = "middle 90% credible intervals",
         text.col = "darkturquoise")
  
  box()
  
  # for Y_pred
  y_lower <- Y_pred_CI_lower[n,]
  y_upper <- Y_pred_CI_upper[n,]
  
  plot(x = x,
       y = c(min(y_lower),
             max(y_upper),
             min(Y_obs[n,]),
             max(Y_obs[n,]),
             rep(min(y_lower), times = no_periods-4)),
       type="l",
       col = "white",
       main = paste("Predicted dependent variable for n =", n),
       xlab = "x",
       ylab = "y")
  
  grid(nx = NA, ny = NULL)
  par(new = TRUE)
  
  polygon(x = c(x, rev(x)),
          y = c(y_lower, rev(y_upper)),
          col = darkturquoise_transp,
          lty = 0)
  
  lines(x = x,
        y = Y_obs[n,],
        lwd = 2,
        col = "orange")
  
  legend("top",
         legend = "middle 90% credible intervals",
         text.col = "darkturquoise")
  
  box()
}
```


# References
* Hartigan, J. A. and Wong, M. A. (1979). Algorithm AS 136: A K-means clustering algorithm. *Applied Statistics*, *28*(1), 100-108. https://doi.org/10.2307/2346830.


