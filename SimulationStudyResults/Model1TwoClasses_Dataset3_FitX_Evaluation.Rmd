---
title: "Table of contents"
output:
  html_document:
    toc: true
    toc_depth: 2
---


```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE)
```


```{r, include = FALSE}
# README
# required files:
# Dataset2_Yobs.xlsx
# Dataset2_zsim.xlsx
# Model1TwoClasses_Dataset2_Fit1.rds
```


# Color coding
* <span style="color: blueviolet;">blueviolet</span> for estimated parameters
* <span style="color: deeppink;">deeppink</span> for simulated parameters
* <span style="color: darkturquoise;">darkturquoise</span> for predicted dependent variable
* <span style="color: orange;">orange</span> for observed dependent variable


# Labeling restriction
$\beta_{0,c} < \beta_{0,c+1}$


# Hyperparameters
* $\mu_{\beta_{0,c}} = \kappa_c$, where $\boldsymbol{\kappa}$ is a row vector of size $C$ storing the estimates of K-means clustering for $\boldsymbol{y}^{obs}_{t=1}$, with $\kappa_c \leq \kappa_{c+1}$; K-means clustering is performed using the algorithm of Hartigan and Wong (1979) with maximum ten iterations and two random sets

* $\sigma_{\beta_{0,c}} = 1$

* $\mu_{\beta_{1,c}} = 0$

* $\sigma_{\beta_{1,c}} = 1$

* $\sigma_{\sigma_c} = 0.5$


# NUTS parameters
* Number of chains: $4$

* Number of iterations per chain: $2000$

* Number of warmup iterations per chain: $1000$

* Number of sampling iterations per chain: $1000$

* Initial values for parameters: random initial values


```{r, include = FALSE}
# preparation ####
# set working directory
setwd("C:/Users/Diiim/Documents/GitHub/BayesianGMMs")

# set decimals to digits instead of scientific
options(scipen = 999)

# load packages
library(readxl)
library(rstan)
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)


# number of latent classes ####
C <- 2


# simulated data ####
# load observed dependent variable
Y_obs <- data.frame(read_excel("SimulationStudyData/Dataset3_Yobs.xlsx",
                               sheet = "Sheet 1"))

# number of individuals
N <- dim(Y_obs)[1]

# number of time periods
no_periods <- dim(Y_obs)[2]

# time periods
time_periods <- 0:(no_periods-1)
X <- matrix(data = time_periods, nrow = N, ncol = no_periods, byrow = TRUE)

# simulated mixture proportions
lambda_sim <- c(0.3,0.7)

# load simulated class memberships
z_sim <- data.frame(read_excel("SimulationStudyData/Dataset3_zsim.xlsx",
                               sheet = "Sheet 1"))

# simulated constants
beta_0_sim <- c(2,3)

# simulated linear trend components
beta_1_sim <- c(-0.5,1)

# simulated standard deviations for Y_obs Normal distributions
sigma_sim <- c(0.25,0.75)


# estimated data ####
# load model fit
m_fit <- readRDS("SimulationStudyResults/Model1TwoClasses_Dataset3_Fit1.rds")

# extract estimated data
m_fit_params <- rstan::extract(m_fit)

# predicted dependent variable
Y_pred <- m_fit_params$Y_pred

# middle 90% credible intervals for Y_pred
Y_pred_CI_lower <- matrix(data = 0, nrow = N, ncol = no_periods)
Y_pred_CI_upper <- matrix(data = 0, nrow = N, ncol = no_periods)
for (t in 1:no_periods) {
  for (n in 1:N) {
    Y_pred_CI_lower[n,t] <- quantile(Y_pred[,n,t], prob = 0.05)
    Y_pred_CI_upper[n,t] <- quantile(Y_pred[,n,t], prob = 0.95)
  }
}


# colors ####
darkturquoise_transp <- rgb(red = 0,
                            green = 206,
                            blue = 209,
                            max = 209,
                            alpha = 50,
                            names = "darkturquoise")
```


# Convergence
## Rhat, Bulk ESS, and Tail ESS
```{r, include = FALSE}
params <- c("lambda[1]","lambda[2]",
            "beta_0[1]","beta_0[2]",
            "beta_1[1]","beta_1[2]",
            "sigma[1]","sigma[2]")
diagnostics <- c("Rhat","Bulk_ESS","Tail_ESS")
Rhat_ESS <-
  as.data.frame(monitor(extract(m_fit, permuted = FALSE)))[params,diagnostics]
```


```{r}
Rhat_ESS
```


## Trace plots
```{r}
# mixture proportions
for (c in 1:C) {
  plot(m_fit_params$lambda[,c],
       type = "l",
       col = "blueviolet",
       main = "",
       xlab = "",
       ylab = "")
  
  grid(nx = NA, ny = NULL)
  par(new = TRUE)
  
  plot(m_fit_params$lambda[,c],
       type = "l",
       col = "blueviolet",
       main = paste("Mixture proportion for c =", c),
       xlab = "Sampling iteration",
       ylab = "lambda")
  
  box()
}
```


```{r}
# constants
for (c in 1:C) {
  plot(m_fit_params$beta_0[,c],
       type = "l",
       col = "blueviolet",
       main = "",
       xlab = "",
       ylab = "")
  
  grid(nx = NA, ny = NULL)
  par(new = TRUE)
  
  plot(m_fit_params$beta_0[,c],
       type = "l",
       col = "blueviolet",
       main = paste("Constant for c =", c),
       xlab = "Sampling iteration",
       ylab = "beta_0")
  
  box()
}
```


```{r}
# linear trend components
for (c in 1:C) {
  plot(m_fit_params$beta_1[,c],
       type = "l",
       col = "blueviolet",
       main = "",
       xlab = "",
       ylab = "")
  
  grid(nx = NA, ny = NULL)
  par(new = TRUE)
  
  plot(m_fit_params$beta_1[,c],
       type = "l",
       col = "blueviolet",
       main = paste("Linear trend component for c =", c),
       xlab = "Sampling iteration",
       ylab = "beta_1")
  
  box()
}
```


```{r}
# standard deviations for Y Normal distributions
for (c in 1:C) {
  plot(m_fit_params$sigma[,c],
       type = "l",
       col = "blueviolet",
       main = "",
       xlab = "",
       ylab = "")
  
  grid(nx = NA, ny = NULL)
  par(new = TRUE)
  
  plot(m_fit_params$sigma[,c],
       type = "l",
       col = "blueviolet",
       main = paste("Standard deviation of Y Normal distributions for c =", c),
       xlab = "Sampling iteration",
       ylab = "sigma")
  
  box()
}
```


# Posterior
```{r}
# mixture proportions
for (c in 1:C) {
  hist(m_fit_params$lambda[,c],
       xlim = c(min(m_fit_params$lambda[,c],lambda_sim[c]),
                max(m_fit_params$lambda[,c],lambda_sim[c])),
       col = "blueviolet",
       border = FALSE,
       main = "",
       xlab = "")
  
  grid()
  par(new = TRUE)

  hist(m_fit_params$lambda[,c],
       xlim = c(min(m_fit_params$lambda[,c],lambda_sim[c]),
                max(m_fit_params$lambda[,c],lambda_sim[c])),
       col = "blueviolet",
       border = FALSE,
       main = paste("Mixture proportion for c =", c),
       xlab = "lambda")
  
  
  abline(v = lambda_sim[c],
         lwd = 2,
         col = "deeppink")
  
  
  box()
}
```


```{r}
# constants
for (c in 1:C) {
  hist(m_fit_params$beta_0[,c],
       xlim = c(min(m_fit_params$beta_0[,c],beta_0_sim[c]),
                max(m_fit_params$beta_0[,c],beta_0_sim[c])),
       col = "blueviolet",
       border = FALSE,
       main = "",
       xlab = "")
  
  grid()
  par(new = TRUE)
  
  hist(m_fit_params$beta_0[,c],
       xlim = c(min(m_fit_params$beta_0[,c],beta_0_sim[c]),
                max(m_fit_params$beta_0[,c],beta_0_sim[c])),
       col = "blueviolet",
       border = FALSE,
       main = paste("Constant for c =", c),
       xlab = "beta_0")
  
  abline(v = beta_0_sim[c],
         lwd = 2,
         col = "deeppink")
  
  box()
}
```


```{r}
# linear trend components
for (c in 1:C) {
  hist(m_fit_params$beta_1[,c],
       xlim = c(min(m_fit_params$beta_1[,c],beta_1_sim[c]),
                max(m_fit_params$beta_1[,c],beta_1_sim[c])),
       col = "blueviolet",
       border = FALSE,
       main = "",
       xlab = "")
  
  grid()
  par(new = TRUE)
  
  hist(m_fit_params$beta_1[,c],
       xlim = c(min(m_fit_params$beta_1[,c],beta_1_sim[c]),
                max(m_fit_params$beta_1[,c],beta_1_sim[c])),
       col = "blueviolet",
       border = FALSE,
       main = paste("Linear trend component for c =", c),
       xlab = "beta_1")
  
  abline(v = beta_1_sim[c],
         lwd = 2,
         col = "deeppink")
  
  box()
}
```


```{r}
# standard deviations for Y Normal distributions
for (c in 1:C) {
  hist(m_fit_params$sigma[,c],
     xlim = c(min(m_fit_params$sigma[,c],sigma_sim[c]),
              max(m_fit_params$sigma[,c],sigma_sim[c])),
     col = "blueviolet",
     border = FALSE,
     main = "",
     xlab = "")
  
  grid()
  par(new = TRUE)
  
  hist(m_fit_params$sigma[,c],
       xlim = c(min(m_fit_params$sigma[,c],sigma_sim[c]),
                max(m_fit_params$sigma[,c],sigma_sim[c])),
       col = "blueviolet",
       border = FALSE,
       main = paste("Standard deviation of Y Normal distributions for c =", c),
       xlab = "sigma")
  
  abline(v = sigma_sim[c],
         lwd = 2,
         col = "deeppink")
  
  box()
}
```


# Posterior predictive check via middle 90% credible interval
```{r}
par(mfrow = c(1,2))
for (n in 1:N) {
  # middle 90% credible interval
  x <- X[n,]
  y_lower <- Y_pred_CI_lower[n,]
  y_upper <- Y_pred_CI_upper[n,]
  
  plot(x = x,
       y = c(min(y_lower),
             max(y_upper),
             min(Y_obs[n,]),
             max(Y_obs[n,]),
             rep(min(y_lower), times = no_periods-4)),
       type="l",
       col = "white",
       main = paste("n =", n),
       xlab = "x",
       ylab = "y")
  
  grid(nx = NA, ny = NULL)
  par(new = TRUE)
  
  polygon(x = c(x, rev(x)),
          y = c(y_lower, rev(y_upper)),
          col = darkturquoise_transp,
          lty = 0)
  
  lines(x = x,
        y = Y_obs[n,],
        lwd = 2,
        col = "orange")
  
  box()

  # class membership
  barplot(prop.table(table(m_fit_params$z[,n])),
       col = "blueviolet",
       border = FALSE,
       main = "",
       xlab = "",
       ylab = "")
  
  grid()
  par(new = TRUE)
  
  barplot(prop.table(table(m_fit_params$z[,n])),
       col = "blueviolet",
       border = FALSE,
       main = "Latent class membership",
       xlab = "c",
       ylab = "Pr( z = c )")
  
  legend("topright",
         legend = paste("z_sim =", z_sim[n,]),
         text.col = "deeppink")
  
  box()
}

par(mfrow = c(1,1))
```


# References
* Hartigan, J. A. and Wong, M. A. (1979). Algorithm AS 136: A K-means clustering algorithm. *Applied Statistics*, *28*(1), 100-108. https://doi.org/10.2307/2346830.


